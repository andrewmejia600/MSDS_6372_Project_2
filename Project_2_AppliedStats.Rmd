---
title: "Applied Statistics 6372 Project 2"
author: "Rajesh satluri, Andrew Mejia and William Hinton"
date: "3/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:



# Introduction

The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.
sification goal is to predict if the client will subscribe a term deposit (variable y).



## Dataset
There are four datasets:
1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]
2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.
3) bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs).
4) bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs).
The smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM).

The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).

Attribute Information:

Input variables:

# bank client data:

1 - age (numeric)
2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')
3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)
4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')
5 - default: has credit in default? (categorical: 'no','yes','unknown')
6 - housing: has housing loan? (categorical: 'no','yes','unknown')
7 - loan: has personal loan? (categorical: 'no','yes','unknown')

# related with the last contact of the current campaign:

8 - contact: contact communication type (categorical: 'cellular','telephone')
9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')
10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')
11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.

# other attributes:

12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
14 - previous: number of contacts performed before this campaign and for this client (numeric)
15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')

# social and economic context attributes

16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
17 - cons.price.idx: consumer price index - monthly indicator (numeric)
18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
20 - nr.employed: number of employees - quarterly indicator (numeric)

Output variable (desired target):
21 - y - has the client subscribed a term deposit? (binary: 'yes','no')


```{r}
## Initialization and data load 
library(knitr)
library(pander)
library(tidyverse)
library(broom)
library(scatterplot3d)
library(DataCombine)
library(corrplot)
library(caret)
library(kableExtra)
library(lubridate)
library(stringr)
library(sjPlot) 
library(gmodels)
library(inspectdf)
library(ggmosaic)
library(cowplot)



library(rms)
library(ggplot2)
library(GGally)
library(dplyr)

library(plotly)

#install.packages("ResourceSelection")
library(ResourceSelection)
library(car)
library(ROCR)
library(MASS)
library(glmnet)
library(bestglm)
#Bank_mrkt_data  = read.csv(file.choose(),header = TRUE, sep = ';')

Bank_mrkt_data  = read.csv("https://raw.githubusercontent.com/andrewmejia600/MSDS_6372_Project_2/master/RAW_DATA/bank-additional-full.csv",header = TRUE, sep = ';')

```





#Common plot functions
```{r, echo=TRUE}

# Defining Plot Function

catplot <- function(df, x,y){
  ggplot(data = df, aes_string(x = x, fill = y)) + 
    geom_bar(position = "fill", alpha = 0.9) + 
    coord_flip()
}
# setting default parameters for crosstables
fun_crosstable = function(df, var1, var2){
  # df: dataframe containing both columns to cross
  # var1, var2: columns to cross together.
  CrossTable(df[, var1], df[, var2],
             prop.r = T,
             prop.c = F,
             prop.t = F,
             prop.chisq = F,
             dnn = c(var1, var2))
}

```


```{r}
dim(Bank_mrkt_data)
```
dataset 41188 rows and 21 columns


```{r}
glimpse(Bank_mrkt_data)

```
First 20 variables are potential predictors and 21st variable 'y' is the response variable

```{r}
## Checking dependent variable

prop.table(table(Bank_mrkt_data$y))

##NOTES:
# An implementation of a cross-tabulation function with output similar to S-Plus crosstabs() and SAS Proc Freq with Chi-square, Fisher and McNemar tests of the independence of all table factors

CrossTable(Bank_mrkt_data$y)


y_pie = Bank_mrkt_data%>%group_by(y)%>%dplyr::summarize(count=n(),  y_percent = n()/nrow(Bank_mrkt_data))
ggplot(data=y_pie,aes(x=2,y=y_percent,fill=y))+
  geom_col(color="white")+
  coord_polar("y",start = 1)+
  geom_text(aes(label=paste0(round(y_percent*100),"%")),
            position = position_stack(vjust = 0.5))+
  theme(panel.background = element_blank(),
        axis.line = element_blank(),
        axis.title = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank(),
        plot.title = element_text(hjust = 0.5,size = I(19)))+
  ggtitle("Percent Of y")+
xlim(.75,2.5)
```
Resampling is required as it is imbalance two level categorical variable.

```{r}

Bank_mrkt_data %>% summarise_all(list(~sum(. == "unknown"))) %>% gather(key = "variable", value = "unknown_rows") %>%   arrange(-unknown_rows)


```
default has maximum number of unknowns, next comes education, housing, loan, job and marital. Totally six predictors has unknowns. As unknown observations are huge, before we decide them to delete we will find the significant predictors.


# EDA and Feature selection

##Categorical variables and their categories distribution

```{r}
## Inspect Categorical Levels
bank_ins_cat=inspect_cat(Bank_mrkt_data)

bank_ins_cat
bank_ins_cat$levels
show_plot(bank_ins_cat)
```





##Categorical variables and their categories distribution by RESPONSE 'y' -  !!IMPORTANT
```{r}
# select categorical variables
categorical_variables = Bank_mrkt_data %>% select_if(is.factor) %>% names()


# remove the response
response_ind <- match('y', categorical_variables)
categorical_variables <- categorical_variables[-response_ind]


# plot categorical variables
for (i in categorical_variables) {

print(i)
  CrossTable(Bank_mrkt_data[, i], Bank_mrkt_data[,"y"],
             prop.r = T,
             prop.c = F,
             prop.t = F,
             prop.chisq = F,
             dnn = c(i, "y"))

  
}

catplot_Categorical <- function(df, x,y){
  ggplot(data = df, aes_string(x = x, fill = y)) + 
    geom_bar(position = "fill", alpha = 0.9) + 
    coord_flip()
}

# Lapply on all categorical_variables values and assing to list object
categorical_variables_plot_list <-lapply(categorical_variables, function(x) catplot_Categorical(Bank_mrkt_data %>% keep(is.factor) ,x, "y"))
# output plot grid
library(cowplot)
plot_grid(plotlist = categorical_variables_plot_list)

#Education
Bank_mrkt_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, education), fill = y)) +
  theme(axis.text.x = element_text(angle = 90,hjust = 1,vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank()) +
  xlab("Education") +
   ylab("y")


#default
Bank_mrkt_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, default), fill = y)) +
  theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank()) +
  xlab("Default") +
   ylab("y")

#housing
Bank_mrkt_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, housing), fill = y)) +
  theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank()) +
  xlab("Housing") +
   ylab("y")


#loan
Bank_mrkt_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, loan), fill = y)) +
  theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank()) +
  xlab("Loan") +
   ylab("y")

#contact
Bank_mrkt_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, contact), fill = y)) +
  theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank()) +
  xlab("Contact") +
   ylab("y")

#month
Bank_mrkt_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, month), fill = y)) +
  theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank()) +
  xlab("Month") +
   ylab("y")


#day_of_week
Bank_mrkt_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, day_of_week), fill = y)) +
  theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank()) +
  xlab("Day of Week") +
  ylab("y")


#poutcome
Bank_mrkt_data %>% 
  ggplot() +
  geom_mosaic(aes(x = product(y, poutcome), fill = y)) +
  theme(axis.text.x = element_text(angle = 90,
                                                hjust = 1,
                                                vjust = 0.5),
                     axis.text.y = element_blank(),
                     axis.ticks.y = element_blank()) +
  xlab("Poutcome") +
  ylab("y")




```

```{r}
# Inspect Continious variables
head(Bank_mrkt_data)
bank_ins_con = inspect_num(Bank_mrkt_data)


## Inspect previous. Clients contacted previously
table(Bank_mrkt_data$previous)

##tally is a convenient wrapper for summarise that will either call n or sum(n) depending on whether you're tallying for the first time, or #re-tallying. count() is similar, but also does the group_by for you.
Bank_mrkt_data %>% group_by(pdays) %>% tally() %>% arrange(desc(n)) %>% head()



# Find Correlation between prevous and pdays
cor(Bank_mrkt_data$previous, Bank_mrkt_data$pdays)  ## Note these are correlated. we may drop pdays
```


##Continious variables and covariance

```{r}

# correlaion between duration and response y. There is relation here.


ggplot(Bank_mrkt_data %>% group_by(duration, y) %>%  tally(), 
       aes(duration, n, fill = y)) +
      geom_col() +
      theme_bw()

ggplot(Bank_mrkt_data, aes(duration, fill = y)) +
  geom_density(alpha = 0.5) +
  theme_bw()

# Plot above 1500 seconds

Bank_mrkt_data_greaterthan_1500 = Bank_mrkt_data %>% filter(duration>=1500)

ggplot(Bank_mrkt_data_greaterthan_1500 %>% group_by(duration, y) %>%  tally(), 
       aes(duration, n, fill = y)) +
      geom_col() +
      theme_bw()

ggplot(Bank_mrkt_data_greaterthan_1500, aes(duration, fill = y)) +
  geom_density(alpha = 0.5) +
  theme_bw()



Bank_mrkt_data_greaterthan_3000 = Bank_mrkt_data %>% filter(duration>=3000)

ggplot(Bank_mrkt_data_greaterthan_3000 %>% group_by(duration, y) %>%  tally(), 
       aes(duration, n, fill = y)) +
      geom_col() +
      theme_bw()

ggplot(Bank_mrkt_data_greaterthan_3000, aes(duration, fill = y)) +
  geom_density(alpha = 0.5) +
  theme_bw()
```


```{r}

## Compare contious variables 
cor_numericVal = inspect_cor(Bank_mrkt_data)

cor_numericVal1 <- select_if(Bank_mrkt_data, is.numeric) %>% cor()
corrplot(cor_numericVal1, method = "number")

corrplot(cor_numericVal1, method = "square",  tl.srt = 50, tl.col = "black", tl.cex = 0.6, title = "Correlation of Variables", mar=c(0,0,1,0))
 corrplot(cor_numericVal1, method = "number",
           type = "upper",
           tl.cex = 0.8,
           tl.srt = 45,
           tl.col = "black")


```




```{r}


#Test Train Splits 

#Down Sampling 
set.seed(1234)
Bank_mrkt_data_dwn_sample = downSample(Bank_mrkt_data[,1:20], Bank_mrkt_data[,21])

#Controling Sampling by modifying the p in the create partition for creating the test train splits from the downsampled data for training. 
set.seed(1234)
Bank_mrkt_data_partition_idx = createDataPartition(Bank_mrkt_data_dwn_sample$Class, p=.5, list = FALSE, times = 1)


Bank_mrkt_data_partition_train = Bank_mrkt_data_dwn_sample[Bank_mrkt_data_partition_idx,]
Bank_mrkt_data_partition_test = Bank_mrkt_data_dwn_sample[-Bank_mrkt_data_partition_idx,]

#Removing Row Number Partitions indices 
row.names(Bank_mrkt_data_partition_train) = NULL
row.names(Bank_mrkt_data_partition_test) = NULL 



#LDA on continous variables --- Highest Accuracy 

lda_analysis = lda(Class ~ age +  duration + campaign + pdays + previous + emp.var.rate + cons.price.idx + cons.conf.idx + euribor3m + nr.employed, data = Bank_mrkt_data_partition_train)

pred = predict(lda_analysis,newdata=Bank_mrkt_data_partition_test)$class
Truth = Bank_mrkt_data_partition_test$Class

table(pred)

confusionMatrix(table(pred, Truth))



```

PCA Analysis 
```{r}
#Balanced test train split
pairs(Bank_mrkt_data_partition_train[, c(1,11,12,13,14,16,17,18,19,20)], col = Bank_mrkt_data_partition_train$Class)

#Not balanced, only 11% is yes 
pairs(Bank_mrkt_data[, c(1,11,12,13,14,16,17,18,19,20)], col = Bank_mrkt_data$y)


#promising 
bank_train_reduced = Bank_mrkt_data[,c(1,11,12,16)]

#Also promising correlated vars 
#bank_train_reduced = Bank_mrkt_data[,c(1,11,16)]

pairs(bank_train_reduced, col = Bank_mrkt_data$y)

pc.result = prcomp(bank_train_reduced,scale.=TRUE)
pc.scores = pc.result$x

pc.result
pairs(pc.scores)


```

```{r}

par(mfrow=c(1,2))
eigenvals<-(pc.result$sdev)^2
plot(1:dim(bank_train_reduced)[2],eigenvals/sum(eigenvals),type="l",main="Scree Plot",ylab="Prop. Var. Explained")
cumulative.prop<-cumsum(eigenvals/sum(eigenvals))
plot(1:dim(bank_train_reduced)[2],cumulative.prop,type="l",main="Cumulative proportion",ylim=c(0,1))


```

```{r}
par(mfrow=c(1,1))

##Minimal PCAs needed to have cumulative explained variance higher than .9

minimal.pca <- which(cumulative.prop>0.9)[1]
minimal.pca
```

Ading the response column to the PCs 
```{r}
pc.scores = data.frame(pc.scores)
#pc.scores$Class = Bank_mrkt_data_partition_train$Class
pc.scores$Class = Bank_mrkt_data$y

ggplot(data = pc.scores, aes(x = PC1, y = PC2 )) + geom_point(aes(col=pc.scores$Class), size = 1, shape = '*') + ggtitle("PCA of Deposits ")

ggplot(data = pc.scores, aes(x = PC2, y = PC3 )) + geom_point(aes(col=pc.scores$Class), size = 1, shape = '*') + ggtitle("PCA of Deposits ")


ggplot(data = pc.scores, aes(x = PC3, y = PC4 )) + geom_point(aes(col=pc.scores$Class), size = 1, shape = '*') + ggtitle("PCA of Deposits ")

```

#Logistic Regression Full list of variables -- HL GOF test pvalue < 2.2e-16 not a good fit
#Need to use for forward selection 

```{r}


main.logr<-glm(Class ~ age + job + marital + education + default + housing + loan + contact + month + day_of_week + duration + campaign + pdays + previous + poutcome + emp.var.rate + cons.price.idx + cons.conf.idx + euribor3m + nr.employed ,data=Bank_mrkt_data_partition_train ,family=binomial(link='logit') )

summary(main.logr)
exp(cbind("Odds ratio" = coef(main.logr), confint.default(main.logr, level = 0.95)))
#vif(logr)

plot(main.logr, pch = 16)

main.logr$aic


hoslem.test(main.logr$y, fitted(main.logr), g=10)

pred = predict(main.logr, newdata = Bank_mrkt_data_partition_test, type="response")

Bank_mrkt_data_partition_test['Pred_Out'] = as.factor(ifelse(pred>.50, "yes", "no"))

Truth = Bank_mrkt_data_partition_test$Class
Predt = Bank_mrkt_data_partition_test$Pred_Out

table(Predt)

confusionMatrix(table(Predt, Truth))

```



#Winning Base Model 
```{r}

main.logr_l_2 = glm(Class ~ duration + cons.price.idx + contact + poutcome,data=Bank_mrkt_data_partition_train ,family=binomial(link='logit'))


#Looking at influence points with LRM model to look at residuals 

resid_lev_fit = lrm(Class ~ duration + cons.price.idx + contact + poutcome,data=Bank_mrkt_data_partition_train, x = TRUE,y = TRUE )

#See too many observations to exclude in different variables, creates a highly biased model
if_point = which.influence(resid_lev_fit, cutoff = 0.0055)

step(main.logr_l_2,
     scope = list(upper=main.logr),
     direction="forward",
     test="Chisq",
     data=Bank_mrkt_data_partition_train)
  
plot(main.logr_l_2)

exp(cbind("Odds ratio" = coef(main.logr_l_2), confint.default(main.logr_l_2, level = 0.95)))

coefficients(main.logr_l_2)

main.logr_l_2

summary(main.logr_l_2)

hoslem.test(main.logr_l_2$y, fitted(main.logr_l_2), g=10)

main.logr_l_2$aic

pred = predict(main.logr_l_2, newdata = Bank_mrkt_data_partition_test, type="response")

Bank_mrkt_data_partition_test['Pred_Out'] = as.factor(ifelse(pred>.50, "yes", "no"))

Truth = Bank_mrkt_data_partition_test$Class
Predt = Bank_mrkt_data_partition_test$Pred_Out

table(Predt)

confusionMatrix(table(Predt, Truth))



```

# L3_is final model for prediction with highest accuracy and lowest AIC yields best AUC results 
#Good overall prediction score especially when looking at ROC curves not much gain in complex models, pretty good accuracy in truth tables for main model-- not a good fit to data -- Removing outliers did not yield much gain in model performance. 
```{r}


main.logr_l_3 = glm(Class ~ duration + month + emp.var.rate + cons.price.idx + contact + poutcome + campaign + euribor3m + cons.conf.idx + job + education ,data=Bank_mrkt_data_partition_train ,family=binomial(link='logit'))

#Looking at influence and how many observations to exclude, still would create a highly biased model 

resid_lev_fit = lrm(Class ~ duration + month + emp.var.rate + cons.price.idx + contact + poutcome + campaign + euribor3m + cons.conf.idx + job + education,data=Bank_mrkt_data_partition_train, x = TRUE,y = TRUE )

if_point = which.influence(resid_lev_fit, cutoff = 0.01)


step(main.logr_l_3,
     scope = list(upper=main.logr),
     direction="forward",
     test="Chisq",
     data=Bank_mrkt_data_partition_train)

exp(cbind("Odds ratio" = coef(main.logr_l_3), confint.default(main.logr_l_3, level = 0.95)))

main.logr_l_3$aic

summary(main.logr_l_3)

coefficients(main.logr_l_3)

hoslem.test(main.logr_l_3$y, fitted(main.logr_l_3), g=10)
plot(main.logr_l_3)

pred = main.logr_l_3$fitted.values

Bank_mrkt_data_partition_train['Pred_Out'] = as.factor(ifelse(pred>.50, "yes", "no"))

Truth = Bank_mrkt_data_partition_train$Class
Predt = Bank_mrkt_data_partition_train$Pred_Out

confusionMatrix(table(Predt, Truth))



#validation against test split
  

pred = predict(main.logr_l_3, newdata = Bank_mrkt_data_partition_test, type="response")

Bank_mrkt_data_partition_test['Pred_Out'] = as.factor(ifelse(pred>.50, "yes", "no"))

Truth = Bank_mrkt_data_partition_test$Class
Predt = Bank_mrkt_data_partition_test$Pred_Out

table(Predt)

confusionMatrix(table(Predt, Truth))




```

#Complex Model 
```{r}
logr.model.complex = glm(Class ~ duration + month + emp.var.rate + cons.price.idx + contact + poutcome + campaign + euribor3m + cons.conf.idx + job + education + duration:education + duration:job ,data=Bank_mrkt_data_partition_train ,family=binomial(link='logit'))
step(main.logr_l_3,
     scope = list(upper=logr.model.complex),
     direction="forward",
     test="Chisq",
     data=Bank_mrkt_data_partition_train)
hoslem.test(logr.model.complex$y, fitted(logr.model.complex), g=10)

logr.model.complex$aic

summary(logr.model.complex)

plot(logr.model.complex)

logr.model.complex$aic

pred = predict(logr.model.complex, newdata = Bank_mrkt_data_partition_test, type="response")

Bank_mrkt_data_partition_test['Pred_Out'] = as.factor(ifelse(pred>.50, "yes", "no"))

Truth = Bank_mrkt_data_partition_test$Class
Predt = Bank_mrkt_data_partition_test$Pred_Out

table(Predt)

confusionMatrix(table(Predt, Truth))


```



order the parameter significance
```{r}
  # 
  # gg1 = varImp(logr.model.complex) %>%  rownames_to_column() %>%    rename(variable = rowname) %>%  arrange(-Overall) %>% 
  #   slice(1:floor(nrow(.))) %>% 
  #   ggplot() +
  #   aes(x = reorder(variable, Overall), weight = Overall, fill = -Overall) +
  #   geom_bar() +
  #   coord_flip() +
  #   xlab("Variables") +
  #   ylab("Importance") +
  #   theme(legend.position = "none")
  # 
  # imp_range = ggplot_build(gg1)[["layout"]][["panel_params"]][[1]][["x.range"]]
  # imp_gradient = scale_fill_gradient(limits = c(-imp_range[2], -imp_range[1]),
  #                                    low = "#132B43", 
  #                                    high = "#56B1F7")
  # gg1
 
```

#ROC Curves need to be in GLMNET inputs matrices 
```{r}
#Base Model AUC results 
Bank_mrkt_data_partition_formula = as.formula(Class ~ duration  + cons.price.idx + contact + poutcome)

Bank_mrkt_data_partition_train_mat.x = model.matrix(Bank_mrkt_data_partition_formula, Bank_mrkt_data_partition_train)
Bank_mrkt_data_partition_train_mat.y = as.matrix(Bank_mrkt_data_partition_train[,21], ncol=1) 

Bank_mrkt_data_partition_test_mat.x = model.matrix(Bank_mrkt_data_partition_formula, Bank_mrkt_data_partition_test)
Bank_mrkt_data_partition_test_mat.y = as.matrix(Bank_mrkt_data_partition_test[,21], ncol=1) 


log_glmnet = glmnet(Bank_mrkt_data_partition_train_mat.x, Bank_mrkt_data_partition_train_mat.y, family = "binomial")

plot(log_glmnet)

coef(log_glmnet)

cvfit =  cv.glmnet(Bank_mrkt_data_partition_train_mat.x, Bank_mrkt_data_partition_train_mat.y, family = "binomial", type.measure = "class", nlambda = 1000)

plot(cvfit)
coef(cvfit, s = "lambda.min")


fit.pred = predict(cvfit, newx =  Bank_mrkt_data_partition_train_mat.x, type = "response")

pred  = prediction(fit.pred[,1], Bank_mrkt_data_partition_train_mat.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train = performance(pred, measure = "auc")
auc.train =  auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))



fit.pred.test = predict(cvfit, newx =  Bank_mrkt_data_partition_test_mat.x, type = "response")

pred.test  = prediction(fit.pred.test[,1], Bank_mrkt_data_partition_test_mat.y)
roc.perf = performance(pred.test, measure = "tpr", x.measure = "fpr")
auc.train = performance(pred.test, measure = "auc")
auc.train =  auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))

```

```{r}


#Winning model AUC results 
Bank_mrkt_data_partition_formula = as.formula(Class ~ duration + month + emp.var.rate + cons.price.idx + contact + poutcome + campaign + euribor3m + cons.conf.idx + job + education)
Bank_mrkt_data_partition_train_mat.x = model.matrix(Bank_mrkt_data_partition_formula, Bank_mrkt_data_partition_train)
Bank_mrkt_data_partition_train_mat.y = as.matrix(Bank_mrkt_data_partition_train[,21], ncol=1) 

Bank_mrkt_data_partition_test_mat.x = model.matrix(Bank_mrkt_data_partition_formula, Bank_mrkt_data_partition_test)
Bank_mrkt_data_partition_test_mat.y = as.matrix(Bank_mrkt_data_partition_test[,21], ncol=1) 


log_glmnet = glmnet(Bank_mrkt_data_partition_train_mat.x, Bank_mrkt_data_partition_train_mat.y, family = "binomial")

plot(log_glmnet)

coef(log_glmnet)

cvfit =  cv.glmnet(Bank_mrkt_data_partition_train_mat.x, Bank_mrkt_data_partition_train_mat.y, family = "binomial", type.measure = "class", nlambda = 1000)

plot(cvfit)
coef(cvfit, s = "lambda.min")


fit.pred = predict(cvfit, newx =  Bank_mrkt_data_partition_train_mat.x, type = "response")

pred  = prediction(fit.pred[,1], Bank_mrkt_data_partition_train_mat.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train = performance(pred, measure = "auc")
auc.train =  auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))



fit.pred.test = predict(cvfit, newx =  Bank_mrkt_data_partition_test_mat.x, type = "response")

pred.test  = prediction(fit.pred.test[,1], Bank_mrkt_data_partition_test_mat.y)
roc.perf = performance(pred.test, measure = "tpr", x.measure = "fpr")
auc.train = performance(pred.test, measure = "auc")
auc.train =  auc.train@y.values

#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))





```

```{r}
#McNemar's Test for competing models 

#Base model vs winning model 
performance_comp_mat_bse_model_winning = matrix(c(2573, 2067,2282,2358), nrow = 2, dimnames = list("BMdl" = c("No", "Yes"), "WMdl"= c("No", "Yes")))

mcnemar.test(performance_comp_mat_bse_model_winning)


#Complex Model vs winning Model 
performance_comp_mat_compl_model_winning = matrix(c(2277,2363,2282,2358), nrow = 2, dimnames = list("CMdl" = c("No", "Yes"), "WMdl"= c("No", "Yes")))

mcnemar.test(performance_comp_mat_compl_model_winning)

#LDA Model vs winning Model 
performance_comp_mat_lda_model_winning = matrix(c(2452,2188,2282,2358), nrow = 2, dimnames = list("CMdl" = c("No", "Yes"), "WMdl"= c("No", "Yes")))

mcnemar.test(performance_comp_mat_lda_model_winning)


  
```





